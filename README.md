# Airflow + DBT + Snowflake ELT Pipeline

This project demonstrates a complete modular ELT (Extract, Load, Transform) data pipeline using Apache Airflow, dbt, and Snowflake. The pipeline automates raw data staging, transformation, and testing using dbt models orchestrated by Airflow DAGs.
- **Airflow** for scheduling and DAG orchestration
- **DBT** for modular SQL-based transformations and testing
- **Snowflake** as the cloud data warehouse
---
## Tech Stack

- Apache Airflow (Astro CLI)
- DBT (Data Build Tool)
- Snowflake Cloud Data Platform
- Cosmos (DBT-Airflow integration)
- Python
- SQL 
- Docker (via Astro CLI)

---

## üß≠ Architecture Diagram
<img width="500" height="400" alt="ELT Pipeline Architecture" src="https://github.com/user-attachments/assets/850643bf-c0fc-4e8f-a9da-1f4b14a0b78b" />

## ‚öôÔ∏è Pre-requisites
-  Snowflake account & credentials with correct roles
-  DBT installed on a virtual env and dbt initialized in a project folder
-  Astro CLI installed
-  Docker Desktop running

-----------------
## Implementation:
## Step 1: Setup snowflake environment
Setup the snowflake environment as per the instructions text doc.
## Step 2: Create and activate virtual env venv and then use it to install dbt
Follow instructions in the instructions text document to create and activate a virtual env. Then install dbt
## Step 3: Initialize a dbt project
follow instructions to create and initialize a dbt project. In our example, we are using dbt_first project.
